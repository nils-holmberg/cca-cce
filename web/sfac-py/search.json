[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Python",
    "section": "",
    "text": "Figure 1: Official logo of the Python programming language\n\n\nAnother principle that has guided the design of this course is that participants should gain an understanding of how Python can be used to accomplish fairly common research tasks within the social sciences more effectively and more transparently. Such applications of Python include collecting and analyzing survey data, performing content analyses on textual and visual material, and using charts to visualize results.\n\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "inst-221017-tables.html",
    "href": "inst-221017-tables.html",
    "title": "day 1: data tables analysis",
    "section": "",
    "text": "Reference to this dataset is Horst et al. (2020)."
  },
  {
    "objectID": "inst-221017-tables.html#jupyter-notebooks",
    "href": "inst-221017-tables.html#jupyter-notebooks",
    "title": "day 1: data tables analysis",
    "section": "1 jupyter notebooks",
    "text": "1 jupyter notebooks\nOn the first workshop session day 1 we will ensure that everyone can get access to a working extensible python environment. The Anaconda3 python environment consists of three main tools: 1) Anaconda navigator for handling environments and packages, 2) Spyder for writing python scripts, and 3) Jupyter notebook for executing interactive code blocks. If you dont have access to Anaconda3 on a local computer, you should be able to follow along from section 1.2.1 below using the google colab cloud service instead.\n\n\n\n\nTable 1: workshop section 1.1.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n1.1.1\nanaconda3\nunderstand install location, create and update virtual environments\n\n\n\n1.1.2\njupyter\ndownload course material, open notebooks, edit in web browser\n\n\n\n1.1.3\ntesting\nexecute the hello world example in jupyter python env\n\n\n\nbreak"
  },
  {
    "objectID": "inst-221017-tables.html#basic-syntax",
    "href": "inst-221017-tables.html#basic-syntax",
    "title": "day 1: data tables analysis",
    "section": "2 basic syntax",
    "text": "2 basic syntax\nDownload the following notebook file and save it to a location where you can find it (121.ipynb). Start your Anaconda navigator app and launch Jupyter notebook from your base environment. Navigate to the notebook file and open it in Jupyter notebook that is running in your web browser. Start executing the notebook code blocks by clicking the play button or by pressing the shift + enter keys. Try editing the code cells and observe the output changes.\n\n\n\n\nTable 2: workshop section 1.2.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n14-15\n1.2.1\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n1.2.2\nfunctions\nreusable pieces of code, understand relation to modules\n\n\n\n1.2.3\nfiles, directories\nnavigate the file system, read, write data files\n\n\n\nbreak\n\n\n\n\n\n\n\n\nYou have now started exploring the basic syntax of the python programming language. Continue by trying out some data objects such as list and dictionaries, and some flow control structures using the if, for, and while statements. Next, you can download the notebook for testing functions (122.ipynb). Open the new file using Jupyter in your web browser and try some example of functions. In the same notebook file you will also find some examples of how to read and write data files using python."
  },
  {
    "objectID": "inst-221017-tables.html#structured-data",
    "href": "inst-221017-tables.html#structured-data",
    "title": "day 1: data tables analysis",
    "section": "3 structured data",
    "text": "3 structured data\nLearning the syntax of python takes a lot of practice. In order to proceed more quickly, we will henceforth employ a low-code approach to python in which we focus on manipulating larger code blocks. This will become useful when working with tablular data using the python pandas package, which is the final outcome of day 1 of this workshop. Let’s start by downloading a new notebook file (131.ipynb) and opening it in Jupyter.\n\n\n\n\nTable 3: workshop section 1.3.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n15-16\n1.3.1\ntabular data\nunderstand how to read, write, and process survey data\n\n\n\n1.3.2\nsummarize\naggregate, combine, join operations\n\n\n\n1.3.3\nsubset, substitute\nand reshape, maybe try out ggplot-like package siuba\n\n\n16-17\nbreak\nanaconda, continued\ntry to resolve anaconda, psychopy installation problems"
  },
  {
    "objectID": "inst-221017-tables.html#data-visualization",
    "href": "inst-221017-tables.html#data-visualization",
    "title": "day 1: data tables analysis",
    "section": "4 data visualization",
    "text": "4 data visualization\nHere is a sneak preview of day 2 of the workshop. For a demonstration of a line plot on a polar axis, see Figure 2.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 2: A line plot on a polar axis"
  },
  {
    "objectID": "topics-3-cca-cv.html#headline-2",
    "href": "topics-3-cca-cv.html#headline-2",
    "title": "computer vision (cv)",
    "section": "2 headline 2",
    "text": "2 headline 2"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "resources",
    "section": "",
    "text": "Neuendorf (2017)\n\n\n\n\n\n\n\nKedia & Rasu (2020)\n\n\n\n\n\n\n\nSzeliski (2010)"
  },
  {
    "objectID": "resources.html#websites-and-apps",
    "href": "resources.html#websites-and-apps",
    "title": "resources",
    "section": "websites and apps",
    "text": "websites and apps\n\nhttps://www.python.org/downloads/\nhttps://wiki.python.org/moin/BeginnersGuide\nhttps://www.anaconda.com/products/individual\nhttps://code.visualstudio.com/download\nhttps://github.com/jupyterlab/jupyterlab_app#download\nhttps://trinket.io/home"
  },
  {
    "objectID": "resources.html#online-articles",
    "href": "resources.html#online-articles",
    "title": "resources",
    "section": "online articles",
    "text": "online articles\n\nsome text here Kedia & Rasu (2020)"
  },
  {
    "objectID": "topics-1-tab-vis.html",
    "href": "topics-1-tab-vis.html",
    "title": "data analysis and visualizations",
    "section": "",
    "text": "Using pandas package to process table data."
  },
  {
    "objectID": "topics-1-tab-vis.html#data-visualizations",
    "href": "topics-1-tab-vis.html#data-visualizations",
    "title": "data analysis and visualizations",
    "section": "2 data visualizations",
    "text": "2 data visualizations\nUsing packages to visualize data."
  },
  {
    "objectID": "topics-2-cca-nlp.html#headline-2",
    "href": "topics-2-cca-nlp.html#headline-2",
    "title": "natural language processing (nlp)",
    "section": "2 headline 2",
    "text": "2 headline 2"
  },
  {
    "objectID": "inst-221020-nlp-tc.html#topic-modelling",
    "href": "inst-221020-nlp-tc.html#topic-modelling",
    "title": "day 4: text classification",
    "section": "1 topic modelling",
    "text": "1 topic modelling\n\n411.ipynb\n412.ipynb\n413.ipynb\n\n\n\n\n\nTable 1: workshop section 4.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n4.1.1\ntf-idf analysis\ndata structures, text frequency, inverse document frequecy\n\n\n\n4.1.2\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n4.1.3\ntopic modelling\ndocument clusters, applying unsupervised learning to texts\n\n\n\nbreak"
  },
  {
    "objectID": "inst-221020-nlp-tc.html#word-vectors",
    "href": "inst-221020-nlp-tc.html#word-vectors",
    "title": "day 4: text classification",
    "section": "2 word vectors",
    "text": "2 word vectors\n\n421.ipynb\n422.ipynb\n423.ipynb\n\n\n\n\n\nTable 2: workshop section 4.2\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n14-15\n4.2.1\nnamed entities\ntext named entity recognition (ner) using spacy\n\n\n\n4.2.2\ntext similarity\n\n\n\n\n4.2.3\nword vectors\ntime series\n\n\n\nbreak"
  },
  {
    "objectID": "inst-221020-nlp-tc.html#text-classification",
    "href": "inst-221020-nlp-tc.html#text-classification",
    "title": "day 4: text classification",
    "section": "3 text classification",
    "text": "3 text classification\n\n\n\n\nTable 3: workshop section 4.3\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n15-16\n4.3.1\ntext classification\ntext classification using deep learning, neural networks\n\n\n\n4.3.2\nsentiment analysis\nbinary text classification\n\n\n\n4.3.3\ntopic classes\nmulti-class text classification\n\n\n16-17\nbreak\n\n\n\n\n\n\n\n\n\n431.ipynb\n432.ipynb\n433.ipynb"
  },
  {
    "objectID": "inst-221020-nlp-tc.html#some-headline",
    "href": "inst-221020-nlp-tc.html#some-headline",
    "title": "day 4: text classification",
    "section": "4 some headline",
    "text": "4 some headline"
  },
  {
    "objectID": "instructions.html",
    "href": "instructions.html",
    "title": "workshop instructions",
    "section": "",
    "text": "Course participants are encouraged to install a Python environment beforehand. The environment used on this course will be Anaconda3 and it consists of several modules, including support for interacting with Jupyter notebooks, which will be the main format of instruction on the course. If you plan to use a managed laptop from Lund University, please check with IT support how to install Anaconda3 via Software Center.1"
  },
  {
    "objectID": "instructions.html#jupyter-notebooks",
    "href": "instructions.html#jupyter-notebooks",
    "title": "workshop instructions",
    "section": "2 jupyter notebooks",
    "text": "2 jupyter notebooks"
  },
  {
    "objectID": "instructions.html#workshop-instructor",
    "href": "instructions.html#workshop-instructor",
    "title": "workshop instructions",
    "section": "3 workshop instructor",
    "text": "3 workshop instructor\n\n\n\nFigure 1: workshop instructor\n\n\nhttps://portal.research.lu.se/en/persons/nils-holmberg"
  },
  {
    "objectID": "schedule.html#course-overview",
    "href": "schedule.html#course-overview",
    "title": "workshop schedule",
    "section": "course overview",
    "text": "course overview\nPython is a programming language that has gained popularity in all types of data science.\n\n\n\n\nTable 1: Some nice table caption.\n\n\n\n\n\n\n\nCourse date\nCourse topic\nPython packages\n\n\n\n\n2022-10-17\nthe anaconda3 environment, basic python syntax, process table data\njupyter, scipy, pandas\n\n\n2022-10-18\ndata visualization, reproducible data analysis, sharing and collaborating\nseaborn, altair, plotly\n\n\n2022-10-19\ntext analysis, manifest content, data cleaning, tokenization, copora\nnltk, pandas, plotly\n\n\n2022-10-20\ntext analysis, latent content features, sentiment analysis, topic modelling\nscikit-learn, spacy\n\n\n2022-10-21\nimage analysis, latent content features, object recognition, captioning\nopencv, pytorch, tf"
  },
  {
    "objectID": "schedule.html#day-1-221017",
    "href": "schedule.html#day-1-221017",
    "title": "workshop schedule",
    "section": "day 1, 221017",
    "text": "day 1, 221017\nPython is a programming language that has gained popularity in all types of data science.\n\n\n\n\nTable 2: Some nice table caption.\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n1.1.1\nanaconda3\nunderstand install location, create and update virtual environments\n\n\n\n1.1.2\njupyter\ndownload course material, open notebooks, edit in web browser\n\n\n\n1.1.3\ntesting\nexecute the hello world example in jupyter python env\n\n\n\nbreak\n\n\n\n\n14-15\n1.2.1\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n1.2.2\nfunctions\nreusable pieces of code, understand relation to modules\n\n\n\n1.2.3\nfiles, directories\nnavigate the file system, read, write data files\n\n\n\nbreak\n\n\n\n\n15-16\n1.3.1\ntabular data\nunderstand how to read, write, and process survey data\n\n\n\n1.3.2\nsummarize\naggregate, combine, join operations\n\n\n\n1.3.3\nsubset, substitute\nand reshape, maybe try out ggplot-like package siuba\n\n\n16-17\nbreak\nanaconda, continued\ntry to resolve anaconda, psychopy installation problems"
  },
  {
    "objectID": "schedule.html#day-2-221018",
    "href": "schedule.html#day-2-221018",
    "title": "workshop schedule",
    "section": "day 2, 221018",
    "text": "day 2, 221018\nPython is a programming language that has gained popularity in all types of data science.\n\n\n\n\nTable 3: Some nice table caption.\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n2.1.1\ngraphics\nbasic python graphics\n\n\n\n2.1.2\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n2.1.3\nrender plots\nunderstand python for computer vision, pil, opencv, matplotlib\n\n\n\nbreak\n\n\n\n\n14-15\n2.2.1\nplotting\nbasic static plots with matplotlib and seaborn\n\n\n\n2.2.2\nbar charts\n\n\n\n\n2.2.3\nscatter plots\ntime series\n\n\n\nbreak\n\n\n\n\n15-16\n2.3.1\nsome\ncomputational documents using altair\n\n\n\n2.3.2\ntopic\nand reshape, maybe try out ggplot-like package plotnine\n\n\n\n2.3.3\ninteractive\ncreating interactive dashboards with plotly and dash\n\n\n16-17\nbreak"
  },
  {
    "objectID": "schedule.html#day-3-221019",
    "href": "schedule.html#day-3-221019",
    "title": "workshop schedule",
    "section": "day 3, 221019",
    "text": "day 3, 221019\nPython is a programming language that has gained popularity in all types of data science.\n\n\n\n\nTable 4: Some nice table caption.\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n3.1.1\ncreate, read text\npython support for representing texts as data objects\n\n\n\n3.1.2\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n3.1.3\nnltk, spacy\nunderstand python for nlp, natural language toolkit, spacy\n\n\n\nbreak\n\n\n\n\n14-15\n3.2.1\ntokenization\ntext data cleaning, analyzing sentences and words\n\n\n\n3.2.2\nnormalization\nlexicon normalization, stemming and lemmatization\n\n\n\n3.2.3\npart of speech\nsyntactic function of text\n\n\n\nbreak\n\n\n\n\n15-16\n3.3.1\ntext corpus\nmanage large collections of text documents\n\n\n\n3.3.2\nbag of words\nmore on text transformation to useful data structures\n\n\n\n3.3.3\ndoc term matrix\ntransformation of texts into useful data structures\n\n\n16-17\nbreak"
  },
  {
    "objectID": "schedule.html#day-4-221020",
    "href": "schedule.html#day-4-221020",
    "title": "workshop schedule",
    "section": "day 4, 221020",
    "text": "day 4, 221020\nPython is a programming language that has gained popularity in all types of data science.\n\n\n\n\nTable 5: Some nice table caption.\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n4.1.1\ntf-idf analysis\ndata structures, text frequency, inverse document frequecy\n\n\n\n4.1.2\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n4.1.3\ntopic modelling\ndocument clusters, applying unsupervised learning to texts\n\n\n\nbreak\n\n\n\n\n14-15\n4.2.1\nnamed entities\ntext named entity recognition (ner) using spacy\n\n\n\n4.2.2\ntext similarity\n\n\n\n\n4.2.3\nword vectors\ntime series\n\n\n\nbreak\n\n\n\n\n15-16\n4.3.1\ntext classification\ntext classification using deep learning, neural networks\n\n\n\n4.3.2\nsentiment analysis\nbinary text classification\n\n\n\n4.3.3\ntopic classes\nmulti-class text classification\n\n\n16-17\nbreak"
  },
  {
    "objectID": "schedule.html#day-5-221021",
    "href": "schedule.html#day-5-221021",
    "title": "workshop schedule",
    "section": "day 5, 221021",
    "text": "day 5, 221021\nPython is a programming language that has gained popularity in all types of data science.\n\n\n\n\nTable 6: Some nice table caption.\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n5.1.1\ngraphics\nread images\n\n\n\n5.1.2\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n5.1.3\nrender images\nmanipulate images, testing the opencv library\n\n\n\nbreak\n\n\n\n\n14-15\n5.2.1\nimage preprocessing\nusing opencv to normalize image data\n\n\n\n5.2.2\nocr classification\ntesting deep learning using the mnist dataset (mlp)\n\n\n\n5.2.3\nimage class\nfirst convolutional neural networks (cnn)\n\n\n\nbreak\n\n\n\n\n15-16\n5.3.1\nobject recognition\nmulti-class image classification using pytorch, tensorflow\n\n\n\n5.3.2\nlocalization\nlocalize object bounding boxes, find object contours\n\n\n\n5.3.3\nimage captioning\nsummarize image content and actions as text description\n\n\n16-17\nbreak"
  },
  {
    "objectID": "inst-221021-cv-img.html",
    "href": "inst-221021-cv-img.html",
    "title": "day 5: image recognition",
    "section": "",
    "text": "This day of the workshop we will focus on python machine learning tasks in the image domain. Previously we have worked with classifying texts and words, and now we will try to perform similar tasks on image pixels. This type of content analysis can be regarded as a subset of the field of computer vision."
  },
  {
    "objectID": "inst-221021-cv-img.html#working-with-images",
    "href": "inst-221021-cv-img.html#working-with-images",
    "title": "day 5: image recognition",
    "section": "1 working with images",
    "text": "1 working with images\nImage data is highly structured compared to text. An image always contains a specific number of rows and columns (pixel dimensions), and each cell (pixel) contains three normalized values between 0 and 255 (RGB color).\n\n\n\n\nTable 1: workshop section 5.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n5.1.1\ngraphics\nread images\n\n\n\n5.1.2\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n5.1.3\nrender images\nmanipulate images, testing the opencv library\n\n\n\nbreak\n\n\n\n\n\n\n\n\n\n511.ipynb\n512.ipynb\n513.ipynb"
  },
  {
    "objectID": "inst-221021-cv-img.html#image-classification",
    "href": "inst-221021-cv-img.html#image-classification",
    "title": "day 5: image recognition",
    "section": "2 image classification",
    "text": "2 image classification\n\n\n\n\nTable 2: workshop section 5.2\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n14-15\n5.2.1\nimage preprocessing\nusing opencv to normalize image data\n\n\n\n5.2.2\nocr classification\ntesting deep learning using the mnist dataset (mlp)\n\n\n\n5.2.3\nimage class\nfirst convolutional neural networks (cnn)\n\n\n\nbreak\n\n\n\n\n\n\n\n\n\n521.ipynb\n522.ipynb\n523.ipynb"
  },
  {
    "objectID": "inst-221021-cv-img.html#object-recognition",
    "href": "inst-221021-cv-img.html#object-recognition",
    "title": "day 5: image recognition",
    "section": "3 object recognition",
    "text": "3 object recognition\n\n\n\n\nTable 3: workshop section 5.3\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n15-16\n5.3.1\nobject recognition\nmulti-class image classification using pytorch, tensorflow\n\n\n\n5.3.2\nlocalization\nlocalize object bounding boxes, find object contours\n\n\n\n5.3.3\nimage captioning\nsummarize image content and actions as text description\n\n\n16-17\nbreak\n\n\n\n\n\n\n\n\n\n531.ipynb\n532.ipynb\n533.ipynb"
  },
  {
    "objectID": "inst-221021-cv-img.html#some-headline",
    "href": "inst-221021-cv-img.html#some-headline",
    "title": "day 5: image recognition",
    "section": "4 some headline",
    "text": "4 some headline"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#texts-and-nlp",
    "href": "inst-221019-nlp-dc.html#texts-and-nlp",
    "title": "day 3: text data analysis",
    "section": "1 texts and nlp",
    "text": "1 texts and nlp\nIf images (and some tables) represent highly structured data, texts can often be found in the other extreme of the spectrum, consisting of highly unstructured data. This poses some unique challenges with text analysis, and the solutions often deals with some aspect of making texts more structured. … in the first part of this session, we will simply load a collection of texts\n\n\n\n\nTable 1: workshop section 3.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n3.1.1\ncreate, read text\npython support for representing texts as data objects\n\n\n\n3.1.2\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n3.1.3\nnltk, spacy\nunderstand python for nlp, natural language toolkit, spacy\n\n\n\nbreak\n\n\n\n\n\n\n\n\n\n311.ipynb\n312.ipynb\n313.ipynb\n\nInstall and access NLTK and spaCy packages. Read"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#structure-text",
    "href": "inst-221019-nlp-dc.html#structure-text",
    "title": "day 3: text data analysis",
    "section": "2 structure text",
    "text": "2 structure text\n\n321.ipynb\n322.ipynb\n323.ipynb\n\n\n\n\n\nTable 2: workshop section 3.2\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n14-15\n3.2.1\ntokenization\ntext data cleaning, analyzing sentences and words\n\n\n\n3.2.2\nnormalization\nlexicon normalization, stemming and lemmatization\n\n\n\n3.2.3\npart of speech\nsyntactic function of text\n\n\n\nbreak"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#text-collections",
    "href": "inst-221019-nlp-dc.html#text-collections",
    "title": "day 3: text data analysis",
    "section": "3 text collections",
    "text": "3 text collections\n\n331.ipynb\n332.ipynb\n333.ipynb\n\n\n\n\n\nTable 3: workshop section 3.3\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n15-16\n3.3.1\ntext corpus\nmanage large collections of text documents\n\n\n\n3.3.2\nbag of words\nmore on text transformation to useful data structures\n\n\n\n3.3.3\ndoc term matrix\ntransformation of texts into useful data structures\n\n\n16-17\nbreak"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#imports",
    "href": "inst-221019-nlp-dc.html#imports",
    "title": "day 3: text data analysis",
    "section": "4 Imports",
    "text": "4 Imports\nTo start, we must import the necessary libraries: Pandas for data frames and Altair for visualization.\n\nimport pandas as pd\nimport altair as alt"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#renderers",
    "href": "inst-221019-nlp-dc.html#renderers",
    "title": "day 3: text data analysis",
    "section": "5 Renderers",
    "text": "5 Renderers\nDepending on your environment, you may need to specify a renderer for Altair. If you are using JupyterLab, Jupyter Notebook, or Google Colab with a live Internet connection you should not need to do anything. Otherwise, please read the documentation for Displaying Altair Charts."
  },
  {
    "objectID": "inst-221019-nlp-dc.html#data",
    "href": "inst-221019-nlp-dc.html#data",
    "title": "day 3: text data analysis",
    "section": "6 Data",
    "text": "6 Data\nData in Altair is built around the Pandas data frame, which consists of a set of named data columns. We will also regularly refer to data columns as data fields.\nWhen using Altair, datasets are commonly provided as data frames. Alternatively, Altair can also accept a URL to load a network-accessible dataset. As we will see, the named columns of the data frame are an essential piece of plotting with Altair.\nWe will often use datasets from the vega-datasets repository. Some of these datasets are directly available as Pandas data frames:\n\nfrom vega_datasets import data  # import vega_datasets\ncars = data.cars()              # load cars data as a Pandas data frame\ncars.head()                     # display the first five rows\n\n                        Name  Miles_per_Gallon  ...       Year  Origin\n0  chevrolet chevelle malibu              18.0  ... 1970-01-01     USA\n1          buick skylark 320              15.0  ... 1970-01-01     USA\n2         plymouth satellite              18.0  ... 1970-01-01     USA\n3              amc rebel sst              16.0  ... 1970-01-01     USA\n4                ford torino              17.0  ... 1970-01-01     USA\n\n[5 rows x 9 columns]\n\n\nDatasets in the vega-datasets collection can also be accessed via URLs:\n\ndata.cars.url\n\n'https://vega.github.io/vega-datasets/data/cars.json'\n\n\nDataset URLs can be passed directly to Altair (for supported formats like JSON and CSV), or loaded into a Pandas data frame like so:\n\npd.read_json(data.cars.url).head() # load JSON data into a data frame\n\n                        Name  Miles_per_Gallon  ...        Year  Origin\n0  chevrolet chevelle malibu              18.0  ...  1970-01-01     USA\n1          buick skylark 320              15.0  ...  1970-01-01     USA\n2         plymouth satellite              18.0  ...  1970-01-01     USA\n3              amc rebel sst              16.0  ...  1970-01-01     USA\n4                ford torino              17.0  ...  1970-01-01     USA\n\n[5 rows x 9 columns]\n\n\nFor more information about data frames - and some useful transformations to prepare Pandas data frames for plotting with Altair! - see the Specifying Data with Altair documentation.\n\n6.1 Weather Data\nStatistical visualization in Altair begins with “tidy” data frames. Here, we’ll start by creating a simple data frame (df) containing the average precipitation (precip) for a given city and month :\n\ndf = pd.DataFrame({\n    'city': ['Seattle', 'Seattle', 'Seattle', 'New York', 'New York', 'New York', 'Chicago', 'Chicago', 'Chicago'],\n    'month': ['Apr', 'Aug', 'Dec', 'Apr', 'Aug', 'Dec', 'Apr', 'Aug', 'Dec'],\n    'precip': [2.68, 0.87, 5.31, 3.94, 4.13, 3.58, 3.62, 3.98, 2.56]\n})\n\ndf\n\n       city month  precip\n0   Seattle   Apr    2.68\n1   Seattle   Aug    0.87\n2   Seattle   Dec    5.31\n3  New York   Apr    3.94\n4  New York   Aug    4.13\n5  New York   Dec    3.58\n6   Chicago   Apr    3.62\n7   Chicago   Aug    3.98\n8   Chicago   Dec    2.56"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#the-chart-object",
    "href": "inst-221019-nlp-dc.html#the-chart-object",
    "title": "day 3: text data analysis",
    "section": "7 The Chart Object",
    "text": "7 The Chart Object\nThe fundamental object in Altair is the Chart, which takes a data frame as a single argument:\n\nchart = alt.Chart(df)\n\nSo far, we have defined the Chart object and passed it the simple data frame we generated above. We have not yet told the chart to do anything with the data."
  },
  {
    "objectID": "inst-221019-nlp-dc.html#marks-and-encodings",
    "href": "inst-221019-nlp-dc.html#marks-and-encodings",
    "title": "day 3: text data analysis",
    "section": "8 Marks and Encodings",
    "text": "8 Marks and Encodings\nWith a chart object in hand, we can now specify how we would like the data to be visualized. We first indicate what kind of graphical mark (geometric shape) we want to use to represent the data. We can set the mark attribute of the chart object using the the Chart.mark_* methods.\nFor example, we can show the data as a point using Chart.mark_point():\n\nalt.Chart(df).mark_point()\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nHere the rendering consists of one point per row in the dataset, all plotted on top of each other, since we have not yet specified positions for these points.\nTo visually separate the points, we can map various encoding channels, or channels for short, to fields in the dataset. For example, we could encode the field city of the data using the y channel, which represents the y-axis position of the points. To specify this, use the encode method:\n\nalt.Chart(df).mark_point().encode(\n  y='city',\n)\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nThe encode() method builds a key-value mapping between encoding channels (such as x, y, color, shape, size, etc.) to fields in the dataset, accessed by field name. For Pandas data frames, Altair automatically determines an appropriate data type for the mapped column, which in this case is the nominal type, indicating unordered, categorical values.\nThough we’ve now separated the data by one attribute, we still have multiple points overlapping within each category. Let’s further separate these by adding an x encoding channel, mapped to the 'precip' field:\n\nalt.Chart(df).mark_point().encode(\n    x='precip',\n    y='city'\n)\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nSeattle exhibits both the least-rainiest and most-rainiest months!\nThe data type of the 'precip' field is again automatically inferred by Altair, and this time is treated as a quantitative type (that is, a real-valued number). We see that grid lines and appropriate axis titles are automatically added as well.\nAbove we have specified key-value pairs using keyword arguments (x='precip'). In addition, Altair provides construction methods for encoding definitions, using the syntax alt.X('precip'). This alternative is useful for providing more parameters to an encoding, as we will see later in this notebook.\n\nalt.Chart(df).mark_point().encode(\n    alt.X('precip'),\n    alt.Y('city')\n)\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nThe two styles of specifying encodings can be interleaved: x='precip', alt.Y('city') is also a valid input to the encode function.\nIn the examples above, the data type for each field is inferred automatically based on its type within the Pandas data frame. We can also explicitly indicate the data type to Altair by annotating the field name:\n\n'b:N' indicates a nominal type (unordered, categorical data),\n'b:O' indicates an ordinal type (rank-ordered data),\n'b:Q' indicates a quantitative type (numerical data with meaningful magnitudes), and\n'b:T' indicates a temporal type (date/time data)\n\nFor example, alt.X('precip:N').\nExplicit annotation of data types is necessary when data is loaded from an external URL directly by Vega-Lite (skipping Pandas entirely), or when we wish to use a type that differs from the type that was automatically inferred.\nWhat do you think will happen to our chart above if we treat precip as a nominal or ordinal variable, rather than a quantitative variable? Modify the code above and find out!\nWe will take a closer look at data types and encoding channels in the next notebook of the data visualization curriculum."
  },
  {
    "objectID": "inst-221019-nlp-dc.html#data-transformation-aggregation",
    "href": "inst-221019-nlp-dc.html#data-transformation-aggregation",
    "title": "day 3: text data analysis",
    "section": "9 Data Transformation: Aggregation",
    "text": "9 Data Transformation: Aggregation\nTo allow for more flexibility in how data are visualized, Altair has a built-in syntax for aggregation of data. For example, we can compute the average of all values by specifying an aggregation function along with the field name:\n\nalt.Chart(df).mark_point().encode(\n    x='average(precip)',\n    y='city'\n)\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nNow within each x-axis category, we see a single point reflecting the average of the values within that category.\nDoes Seattle really have the lowest average precipitation of these cities? (It does!) Still, how might this plot mislead? Which months are included? What counts as precipitation?\nAltair supports a variety of aggregation functions, including count, min (minimum), max (maximum), average, median, and stdev (standard deviation). In a later notebook, we will take a tour of data transformations, including aggregation, sorting, filtering, and creation of new derived fields using calculation formulas."
  },
  {
    "objectID": "inst-221019-nlp-dc.html#changing-the-mark-type",
    "href": "inst-221019-nlp-dc.html#changing-the-mark-type",
    "title": "day 3: text data analysis",
    "section": "10 Changing the Mark Type",
    "text": "10 Changing the Mark Type\nLet’s say we want to represent our aggregated values using rectangular bars rather than circular points. We can do this by replacing Chart.mark_point with Chart.mark_bar:\n\nalt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city'\n)\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nBecause the nominal field a is mapped to the y-axis, the result is a horizontal bar chart. To get a vertical bar chart, we can simply swap the x and y keywords:\n\nalt.Chart(df).mark_bar().encode(\n    x='city',\n    y='average(precip)'\n)"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#customizing-a-visualization",
    "href": "inst-221019-nlp-dc.html#customizing-a-visualization",
    "title": "day 3: text data analysis",
    "section": "11 Customizing a Visualization",
    "text": "11 Customizing a Visualization\nBy default Altair / Vega-Lite make some choices about properties of the visualization, but these can be changed using methods to customize the look of the visualization. For example, we can specify the axis titles using the axis attribute of channel classes, we can modify scale properties using the scale attribute, and we can specify the color of the marking by setting the color keyword of the Chart.mark_* methods to any valid CSS color string:\n\nalt.Chart(df).mark_point(color='firebrick').encode(\n  alt.X('precip', scale=alt.Scale(type='log'), axis=alt.Axis(title='Log-Scaled Values')),\n  alt.Y('city', axis=alt.Axis(title='Category')),\n)\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nA subsequent module will explore the various options available for scales, axes, and legends to create customized charts."
  },
  {
    "objectID": "inst-221019-nlp-dc.html#multiple-views",
    "href": "inst-221019-nlp-dc.html#multiple-views",
    "title": "day 3: text data analysis",
    "section": "12 Multiple Views",
    "text": "12 Multiple Views\nAs we’ve seen above, the Altair Chart object represents a plot with a single mark type. What about more complicated diagrams, involving multiple charts or layers? Using a set of view composition operators, Altair can take multiple chart definitions and combine them to create more complex views.\nAs a starting point, let’s plot the cars dataset in a line chart showing the average mileage by the year of manufacture:\n\nalt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nTo augment this plot, we might like to add circle marks for each averaged data point. (The circle mark is just a convenient shorthand for point marks that used filled circles.)\nWe can start by defining each chart separately: first a line plot, then a scatter plot. We can then use the layer operator to combine the two into a layered chart. Here we use the shorthand + (plus) operator to invoke layering:\n\nline = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\npoint = alt.Chart(cars).mark_circle().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\nline + point\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nWe can also create this chart by reusing and modifying a previous chart definition! Rather than completely re-write a chart, we can start with the line chart, then invoke the mark_point method to generate a new chart definition with a different mark type:\n\nmpg = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Miles_per_Gallon)')\n)\n\nmpg + mpg.mark_circle()\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\n(The need to place points on lines is so common, the line mark also includes a shorthand to generate a new layer for you. Trying adding the argument point=True to the mark_line method!)\nNow, what if we’d like to see this chart alongside other plots, such as the average horsepower over time?\nWe can use concatenation operators to place multiple charts side-by-side, either vertically or horizontally. Here, we’ll use the | (pipe) operator to perform horizontal concatenation of two charts:\n\nhp = alt.Chart(cars).mark_line().encode(\n    alt.X('Year'),\n    alt.Y('average(Horsepower)')\n)\n\n(mpg + mpg.mark_circle()) | (hp + hp.mark_circle())\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nWe can see that, in this dataset, over the 1970s and early ’80s the average fuel efficiency improved while the average horsepower decreased.\nA later notebook will focus on view composition, including not only layering and concatenation, but also the facet operator for splitting data into sub-plots and the repeat operator to concisely generate concatenated charts from a template."
  },
  {
    "objectID": "inst-221019-nlp-dc.html#interactivity",
    "href": "inst-221019-nlp-dc.html#interactivity",
    "title": "day 3: text data analysis",
    "section": "13 Interactivity",
    "text": "13 Interactivity\nIn addition to basic plotting and view composition, one of Altair and Vega-Lite’s most exciting features is its support for interaction.\nTo create a simple interactive plot that supports panning and zooming, we can invoke the interactive() method of the Chart object. In the chart below, click and drag to pan or use the scroll wheel to zoom:\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n).interactive()\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nTo provide more details upon mouse hover, we can use the tooltip encoding channel:\n\nalt.Chart(cars).mark_point().encode(\n    x='Horsepower',\n    y='Miles_per_Gallon',\n    color='Origin',\n    tooltip=['Name', 'Origin'] # show Name and Origin in a tooltip\n).interactive()\n\n\n\n\n  \n  \n  \n  \n\n\n  \n  \n\n\n\n\nFor more complex interactions, such as linked charts and cross-filtering, Altair provides a selection abstraction for defining interactive selections and then binding them to components of a chart. We will cover this is in detail in a later notebook.\nBelow is a more complex example. The upper histogram shows the count of cars per year and uses an interactive selection to modify the opacity of points in the lower scatter plot, which shows horsepower versus mileage.\nDrag out an interval in the upper chart and see how it affects the points in the lower chart. As you examine the code, don’t worry if parts don’t make sense yet! This is an aspirational example, and we will fill in all the needed details over the course of the different notebooks.\n\n# create an interval selection over an x-axis encoding\nbrush = alt.selection_interval(encodings=['x'])\n\n# determine opacity based on brush\nopacity = alt.condition(brush, alt.value(0.9), alt.value(0.1))\n\n# an overview histogram of cars per year\n# add the interval brush to select cars over time\noverview = alt.Chart(cars).mark_bar().encode(\n    alt.X('Year:O', timeUnit='year', # extract year unit, treat as ordinal\n      axis=alt.Axis(title=None, labelAngle=0) # no title, no label angle\n    ),\n    alt.Y('count()', title=None), # counts, no axis title\n    opacity=opacity\n).add_selection(\n    brush      # add interval brush selection to the chart\n).properties(\n    width=400, # set the chart width to 400 pixels\n    height=50  # set the chart height to 50 pixels\n)\n\n# a detail scatterplot of horsepower vs. mileage\n# modulate point opacity based on the brush selection\ndetail = alt.Chart(cars).mark_point().encode(\n    alt.X('Horsepower'),\n    alt.Y('Miles_per_Gallon'),\n    # set opacity based on brush selection\n    opacity=opacity\n).properties(width=400) # set chart width to match the first chart\n\n# vertically concatenate (vconcat) charts using the '&' operator\noverview & detail"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#aside-examining-the-json-output",
    "href": "inst-221019-nlp-dc.html#aside-examining-the-json-output",
    "title": "day 3: text data analysis",
    "section": "14 Aside: Examining the JSON Output",
    "text": "14 Aside: Examining the JSON Output\nAs a Python API to Vega-Lite, Altair’s main purpose is to convert plot specifications to a JSON string that conforms to the Vega-Lite schema. Using the Chart.to_json method, we can inspect the JSON specification that Altair is exporting and sending to Vega-Lite:\n\nchart = alt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city',\n)\nprint(chart.to_json())\n\n{\n  \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\",\n  \"config\": {\n    \"view\": {\n      \"continuousHeight\": 300,\n      \"continuousWidth\": 400\n    }\n  },\n  \"data\": {\n    \"name\": \"data-fdfbb22e8e0e89f6556d8a3b434b0c97\"\n  },\n  \"datasets\": {\n    \"data-fdfbb22e8e0e89f6556d8a3b434b0c97\": [\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Apr\",\n        \"precip\": 2.68\n      },\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Aug\",\n        \"precip\": 0.87\n      },\n      {\n        \"city\": \"Seattle\",\n        \"month\": \"Dec\",\n        \"precip\": 5.31\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Apr\",\n        \"precip\": 3.94\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Aug\",\n        \"precip\": 4.13\n      },\n      {\n        \"city\": \"New York\",\n        \"month\": \"Dec\",\n        \"precip\": 3.58\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Apr\",\n        \"precip\": 3.62\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Aug\",\n        \"precip\": 3.98\n      },\n      {\n        \"city\": \"Chicago\",\n        \"month\": \"Dec\",\n        \"precip\": 2.56\n      }\n    ]\n  },\n  \"encoding\": {\n    \"x\": {\n      \"aggregate\": \"average\",\n      \"field\": \"precip\",\n      \"type\": \"quantitative\"\n    },\n    \"y\": {\n      \"field\": \"city\",\n      \"type\": \"nominal\"\n    }\n  },\n  \"mark\": \"bar\"\n}\n\n\nNotice here that encode(x='average(precip)') has been expanded to a JSON structure with a field name, a type for the data, and includes an aggregate field. The encode(y='city') statement has been expanded similarly.\nAs we saw earlier, Altair’s shorthand syntax includes a way to specify the type of the field as well:\n\nx = alt.X('average(precip):Q')\nprint(x.to_json())\n\n{\n  \"aggregate\": \"average\",\n  \"field\": \"precip\",\n  \"type\": \"quantitative\"\n}\n\n\nThis short-hand is equivalent to spelling-out the attributes by name:\n\nx = alt.X(aggregate='average', field='precip', type='quantitative')\nprint(x.to_json())\n\n{\n  \"aggregate\": \"average\",\n  \"field\": \"precip\",\n  \"type\": \"quantitative\"\n}"
  },
  {
    "objectID": "inst-221019-nlp-dc.html#publishing-a-visualization",
    "href": "inst-221019-nlp-dc.html#publishing-a-visualization",
    "title": "day 3: text data analysis",
    "section": "15 Publishing a Visualization",
    "text": "15 Publishing a Visualization\nOnce you have visualized your data, perhaps you would like to publish it somewhere on the web. This can be done straightforwardly using the vega-embed JavaScript package. A simple example of a stand-alone HTML document can be generated for any chart using the Chart.save method:\nchart = alt.Chart(df).mark_bar().encode(\n    x='average(precip)',\n    y='city',\n)\nchart.save('chart.html')\nThe basic HTML template produces output that looks like this, where the JSON specification for your plot produced by Chart.to_json should be stored in the spec JavaScript variable:\n<!DOCTYPE html>\n<html>\n  <head>\n    <script src=\"https://cdn.jsdelivr.net/npm/vega@5\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/vega-lite@4\"></script>\n    <script src=\"https://cdn.jsdelivr.net/npm/vega-embed@6\"></script>\n  </head>\n  <body>\n  <div id=\"vis\"></div>\n  <script>\n    (function(vegaEmbed) {\n      var spec = {}; /* JSON output for your chart's specification */\n      var embedOpt = {\"mode\": \"vega-lite\"}; /* Options for the embedding */\n\n      function showError(el, error){\n          el.innerHTML = ('<div style=\"color:red;\">'\n                          + '<p>JavaScript Error: ' + error.message + '</p>'\n                          + \"<p>This usually means there's a typo in your chart specification. \"\n                          + \"See the javascript console for the full traceback.</p>\"\n                          + '</div>');\n          throw error;\n      }\n      const el = document.getElementById('vis');\n      vegaEmbed(\"#vis\", spec, embedOpt)\n        .catch(error => showError(el, error));\n    })(vegaEmbed);\n  </script>\n</body>\n</html>\nThe Chart.save method provides a convenient way to save such HTML output to file. For more information on embedding Altair/Vega-Lite, see the documentation of the vega-embed project."
  },
  {
    "objectID": "inst-221018-visual.html",
    "href": "inst-221018-visual.html",
    "title": "day 2: data visualization",
    "section": "",
    "text": "Reference to this dataset is Horst et al. (2020)."
  },
  {
    "objectID": "inst-221018-visual.html#graphics-and-images",
    "href": "inst-221018-visual.html#graphics-and-images",
    "title": "day 2: data visualization",
    "section": "1 graphics and images",
    "text": "1 graphics and images\n\n211.ipynb\n212.ipynb\n213.ipynb\n\n\n\n\n\nTable 1: workshop section 2.1.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n13-14\n2.1.1\ngraphics\nbasic python graphics\n\n\n\n2.1.2\npython syntax\npractice data structures, conditional statements, flow control\n\n\n\n2.1.3\nrender plots\nunderstand python for computer vision, pil, opencv, matplotlib\n\n\n\nbreak"
  },
  {
    "objectID": "inst-221018-visual.html#basic-static-plots",
    "href": "inst-221018-visual.html#basic-static-plots",
    "title": "day 2: data visualization",
    "section": "2 basic static plots",
    "text": "2 basic static plots\n\n221.ipynb\n222.ipynb\n223.ipynb\n\n\n\n\n\nTable 2: workshop section 2.2.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n14-15\n2.2.1\nplotting\nbasic static plots with matplotlib and seaborn\n\n\n\n2.2.2\nbar charts\n\n\n\n\n2.2.3\nscatter plots\ntime series\n\n\n\nbreak"
  },
  {
    "objectID": "inst-221018-visual.html#interactive-plots",
    "href": "inst-221018-visual.html#interactive-plots",
    "title": "day 2: data visualization",
    "section": "3 interactive plots",
    "text": "3 interactive plots\n\n231.ipynb\n232.ipynb\n233.ipynb\n\n\n\n\n\nTable 3: workshop section 2.3.1\n\n\n\n\n\n\n\n\ntime\nsection\nconcepts\noutcomes\n\n\n\n\n15-16\n2.3.1\nsome\ncomputational documents using altair\n\n\n\n2.3.2\ntopic\nand reshape, maybe try out ggplot-like package plotnine\n\n\n\n2.3.3\ninteractive\ncreating interactive dashboards with plotly and dash\n\n\n16-17\nbreak"
  },
  {
    "objectID": "inst-221018-visual.html#some-headline",
    "href": "inst-221018-visual.html#some-headline",
    "title": "day 2: data visualization",
    "section": "4 some headline",
    "text": "4 some headline\n\n# get altair graphics\nimport altair as alt\n# get data object from vega_datasets\nfrom vega_datasets import data\n\n\n# Selecting the data\ndf = data.iris()\n  \n# Making the Scatter Plot\nalt.Chart(df).mark_point().encode(\n    # Map the sepalLength to x-axis\n    x='sepalLength',\n    # Map the petalLength to y-axis\n    y='petalLength',\n    # Map the species to shape\n    shape='species',\n    #\n    color='species'\n)"
  },
  {
    "objectID": "topics.html",
    "href": "topics.html",
    "title": "workshop topics",
    "section": "",
    "text": "Some data analysis and more data analysis.\n\n\n\nFigure 1: palmer penguins\n\n\nReference to this dataset is Horst et al. (2020)."
  },
  {
    "objectID": "topics.html#computational-content-analysis",
    "href": "topics.html#computational-content-analysis",
    "title": "workshop topics",
    "section": "2 computational content analysis",
    "text": "2 computational content analysis\nFirst aspect of CCA is natural language processing (nlp), while the second aspect is computer vision (cv)."
  }
]