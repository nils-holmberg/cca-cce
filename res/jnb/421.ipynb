{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9481bdf4-012e-40cc-a91f-0333b61941ac",
   "metadata": {},
   "source": [
    "To cluster our corpus, we can choose from several algorithms, including non-negative matrix factorization (NMF), sparse principal components analysis (sparse PCA), and latent dirichlet allocation (LDA). We’ll focus on LDA because it is widely used by the scientific community due to its good results in social media, medical science, political science, and software engineering.\n",
    "\n",
    "LDA is a model for unsupervised topic decomposition: It groups texts based on the words they contain and the probability of a word belonging to a certain topic. The LDA algorithm outputs the topic word distribution. With this information, we can define the main topics based on the words that are most likely associated with them. Once we have identified the main topics and their associated words, we can know which topic or topics apply to each text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a59c16-6cef-4a8c-bb48-8c0040d6684e",
   "metadata": {},
   "source": [
    "![](https://bs-uploads.toptal.io/blackfish-uploads/public-files/image_0-259d7a671398a16dc7cdfe05d89d4880.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aca90609-8461-47b2-9a4a-9ff78a2ee635",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I like Harry Potter', 'I like Star Wars']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"I like Harry Potter\",\n",
    "\"I like Star Wars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9044592-c449-4d20-9cb5-0642f60245b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1, 0, 0], [1, 1, 0, 0, 1, 1]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[1,1,1,1,0,0],\n",
    "[1,1,0,0,1,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed12fb96-5cfc-40d7-8e74-267b022082b4",
   "metadata": {},
   "source": [
    "![](https://bs-uploads.toptal.io/blackfish-uploads/public-files/image_1-0e8ed3e4c4e3de798d821211ae2c0537.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2064f1b-9f26-4831-ba78-a8a7819e9d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc32e8c-58f5-49d8-a149-7256b4411938",
   "metadata": {},
   "source": [
    "Consider the following corpus composed of five short sentences (all taken from New York Times headlines). The algorithm should clearly identify one topic related to politics and coronavirus, and a second one related to Nadal and tennis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "887975bc-5254-4a1f-961f-7efc952b0b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"Rafael Nadal Joins Roger Federer in Missing U.S. Open\",\n",
    "          \"Rafael Nadal Is Out of the Australian Open\",\n",
    "          \"Biden Announces Virus Measures\",\n",
    "          \"Biden's Virus Plans Meet Reality\",\n",
    "          \"Where Biden's Virus Plan Stands\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63007508-fb65-4d8e-905d-d058bbea6a52",
   "metadata": {},
   "source": [
    "For best results, it’s necessary to use multiple preprocessing techniques. Here are some of the most frequently used:\n",
    "\n",
    "- Lowercase letters. Make all words lowercase. Make all words lowercase. The meaning of a word does not change regardless of its position in the sentence.\n",
    "- n-grams. Consider all groups of n words in a row as new terms, called n-grams. This way, cases such as “white house” will be taken into account and added to the vocabulary list.\n",
    "- Stemming. Identify prefixes and suffixes of words to isolate them from their root. This way, words like “play,” “played,” or “player” are represented by the word “play.” Stemming can be useful to reduce the number of words in the vocabulary list while preserving their meaning , but it slows preprocessing considerably because it must be applied to each word in the corpus.\n",
    "- Stop words. Do not take into account groups of words lacking in meaning or utility. These include articles and prepositions but may also include words that are not useful for our specific case study, such as certain common verbs.\n",
    "- Term frequency–inverse document frequency (tf–idf). Use the coefficient of tf–idf instead of noting the frequency of each word within each cell of the matrix. It consists of two numbers, multiplied:\n",
    "\n",
    "  - tf—the frequency of a given term or word in a text, and\n",
    "  - idf—the logarithm of the total number of documents divided by the number of documents that contain that given term.\n",
    "\n",
    "  tf–idf is a measure of how frequently a word is used in the corpus. To be able to subdivide words into groups, it is important to understand not only which words appear in each text, but also which words appear frequently in one text but not at all in others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f61883-fe57-47bd-8fea-4585899e156b",
   "metadata": {},
   "source": [
    "Using CountVectorizer(), we generate the matrix that denotes the frequency of the words of each text using CountVectorizer(). Note that the CountVectorizer allows for preprocessing if you include parameters such as stop_words to include the stop words, ngram_range to include n-grams, or lowercase=True to convert all characters to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0682f7d9-f684-4117-ae06-be0eae9e972f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer(stop_words=stopwords.words('english'), lowercase=True)\n",
    "x_counts = count_vect.fit_transform(corpus)\n",
    "x_counts.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6020d1e1-ba9a-44e0-9c03-09a27f4cb494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['announces', 'australian', 'biden', 'federer', 'joins', 'measures',\n",
       "       'meet', 'missing', 'nadal', 'open', 'plan', 'plans', 'rafael',\n",
       "       'reality', 'roger', 'stands', 'virus'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959572de-4347-473a-b1a4-27f5d663da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "x_tfidf = tfidf_transformer.fit_transform(x_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2890d6-c8ae-4779-a3dd-7cb9b2c554f5",
   "metadata": {},
   "source": [
    "In order to perform the LDA decomposition, we have to define the number of topics. In this simple case, we know there are two topics or “dimensions.” But in general cases, this is a hyperparameter that needs some tuning, which could be done using algorithms like random search or grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc5cacfe-0040-440d-8f14-7a617c2855a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15689346, 0.84310654],\n",
       "       [0.27535307, 0.72464693],\n",
       "       [0.81253354, 0.18746646],\n",
       "       [0.82590169, 0.17409831],\n",
       "       [0.32353416, 0.67646584]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 2\n",
    "lda = LDA(n_components = dimension)\n",
    "lda_array = lda.fit_transform(x_tfidf)\n",
    "lda_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26bb11-370f-410b-8045-8f674d6beb94",
   "metadata": {},
   "source": [
    "LDA is a probabilistic method. Here we can see the probability of each of the five headlines belonging to each of the two topics. We can see that the first two texts have a higher probability of belonging to the first topic and the next three to the second topic, as expected.\n",
    "\n",
    "Finally, if we want to understand what these two topics are about, we can see the most important words in each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3cc00c-6017-4707-a015-53bb9b2d39e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['virus', 'biden', 'announces'], ['rafael', 'open', 'nadal']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components = [lda.components_[i] for i in range(len(lda.components_))]\n",
    "features = count_vect.get_feature_names()\n",
    "important_words = [sorted(features, key = lambda x: components[j][features.index(x)], reverse = True)[:3] for j in range(len(components))]\n",
    "important_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae97ccf-e382-43c6-a1bf-7370e59051aa",
   "metadata": {},
   "source": [
    "As expected, LDA correctly assigned words related to tennis tournaments and Nadal to the first topic and words related to Biden and virus to the second topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "539247ac-a78b-4588-bc03-47ead297183c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b50921ef-73cf-4934-9009-6f713761d19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english',smooth_idf=True) \n",
    "# under the hood - lowercasing,removing special chars,removing stop words\n",
    "input_matrix = vectorizer.fit_transform(corpus).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "86e21113-6e71-49a2-8891-15935912798b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.40986539, 0.40986539,\n",
       "         0.        , 0.        , 0.40986539, 0.33067681, 0.33067681,\n",
       "         0.        , 0.        , 0.33067681, 0.        , 0.40986539,\n",
       "         0.        , 0.        ],\n",
       "        [0.        , 0.5819515 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.4695148 , 0.4695148 ,\n",
       "         0.        , 0.        , 0.4695148 , 0.        , 0.        ,\n",
       "         0.        , 0.        ],\n",
       "        [0.58752141, 0.        , 0.39346994, 0.        , 0.        ,\n",
       "         0.58752141, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.39346994],\n",
       "        [0.        , 0.        , 0.33925099, 0.        , 0.        ,\n",
       "         0.        , 0.50656277, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.50656277, 0.        , 0.50656277, 0.        ,\n",
       "         0.        , 0.33925099],\n",
       "        [0.        , 0.        , 0.39346994, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.58752141, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.58752141, 0.39346994]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "171ff16e-911a-4fac-9ebc-6ebf5b83aab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sol-nhl/lib/python/anaconda3/envs/sfac-py/lib/python3.8/site-packages/sklearn/utils/validation.py:727: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['announces', 'australian', 'biden', 'federer', 'joins', 'measures',\n",
       "       'meet', 'missing', 'nadal', 'open', 'plan', 'plans', 'rafael',\n",
       "       'reality', 'roger', 'stands', 'virus'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_modeling= TruncatedSVD(n_components=2, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_modeling.fit(input_matrix)\n",
    "components=svd_modeling.components_\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6a9a011-410c-4355-adf5-576ef77f2df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['  biden', '  biden virus', '  biden virus measures', '  biden virus measures announces', '  biden virus measures announces plan', '  biden virus measures announces plan stands', '  biden virus measures announces plan stands meet', '  nadal', '  nadal open', '  nadal open rafael', '  nadal open rafael australian', '  nadal open rafael australian federer', '  nadal open rafael australian federer joins', '  nadal open rafael australian federer joins missing']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['  biden',\n",
       " '  biden virus',\n",
       " '  biden virus measures',\n",
       " '  biden virus measures announces',\n",
       " '  biden virus measures announces plan',\n",
       " '  biden virus measures announces plan stands',\n",
       " '  biden virus measures announces plan stands meet',\n",
       " '  nadal',\n",
       " '  nadal open',\n",
       " '  nadal open rafael',\n",
       " '  nadal open rafael australian',\n",
       " '  nadal open rafael australian federer',\n",
       " '  nadal open rafael australian federer joins',\n",
       " '  nadal open rafael australian federer joins missing']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_list = []\n",
    "def get_topics(components): \n",
    "    for i, comp in enumerate(components):\n",
    "        terms_comp = zip(vocab,comp)\n",
    "        sorted_terms = sorted(terms_comp, key= lambda x:x[1], reverse=True)[:7]\n",
    "        topic=\" \"\n",
    "        for t in sorted_terms:\n",
    "            topic= topic + ' ' + t[0]\n",
    "            topic_word_list.append(topic)\n",
    "    print(topic_word_list)\n",
    "    return topic_word_list\n",
    "\n",
    "get_topics(components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2386e-29b2-4382-884e-53f794fa72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(4):\n",
    "    wc = WordCloud(width=1000, height=600, margin=3,  prefer_horizontal=0.7,scale=1,background_color='black', relative_scaling=0).generate(topic_word_list[i])\n",
    "    plt.imshow(wc)\n",
    "    plt.title(f\"Topic{i+1}\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
