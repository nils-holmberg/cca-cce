{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_r-eMlMgOrv",
        "outputId": "3a18da22-513e-499f-b49b-cc44065c38ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download(\"popular\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create a corpus with 2 texts in different textfile.\n",
        "txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\n",
        "txt2 = \"\"\"Are you a foo bar? Yes I am. Possibly, everyone is.\\n\"\"\"\n",
        "corpus = [txt1,txt2]\n",
        "\n",
        "# Make new dir for the corpus.\n",
        "corpusdir = 'tmp/newcorpus/'\n",
        "if not os.path.isdir(corpusdir):\n",
        "    os.mkdir(corpusdir)\n",
        "\n",
        "# Output the files into the directory.\n",
        "filename = 0\n",
        "for text in corpus:\n",
        "    filename+=1\n",
        "    with open(corpusdir+str(filename)+'.txt','w') as fout:\n",
        "        #print>>fout, text\n",
        "        #print(text, fout)\n",
        "        fout.write(text)\n"
      ],
      "metadata": {
        "id": "z0OtxEf_hVW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat newcorpus/1.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Li6wtDUi1TS",
        "outputId": "91335ee1-716d-4e9d-cfdc-d4d3579f4f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a foo bar sentence.\n",
            "And this is the first txtfile in the corpus."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that our corpus do exist and the files are correct.\n",
        "assert os.path.isdir(corpusdir)\n",
        "for infile, text in zip(sorted(os.listdir(corpusdir)),corpus):\n",
        "    assert open(corpusdir+infile,'r').read().strip() == text.strip()\n",
        "\n",
        "\n",
        "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
        "# Create a new corpus by specifying the parameters\n",
        "# (1) directory of the new corpus\n",
        "# (2) the fileids of the corpus\n",
        "# NOTE: in this case the fileids are simply the filenames.\n",
        "newcorpus = PlaintextCorpusReader('newcorpus/', '.*')\n"
      ],
      "metadata": {
        "id": "EfEHvPwDhV3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access each file in the corpus.\n",
        "for infile in sorted(newcorpus.fileids()):\n",
        "    print(infile) # The fileids of each file.\n",
        "    with newcorpus.open(infile) as fin: # Opens the file.\n",
        "        print(fin.read().strip()) # Prints the content of the file\n",
        "\n",
        "# Access the plaintext; outputs pure string/basestring.\n",
        "print(newcorpus.raw().strip())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfIgEXPxhWZC",
        "outputId": "bfee8af6-a937-484c-b1af-f582ef681ad7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.txt\n",
            "This is a foo bar sentence.\n",
            "And this is the first txtfile in the corpus.\n",
            "2.txt\n",
            "Are you a foo bar? Yes I am. Possibly, everyone is.\n",
            "This is a foo bar sentence.\n",
            "And this is the first txtfile in the corpus.Are you a foo bar? Yes I am. Possibly, everyone is.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access paragraphs in the corpus. (list of list of list of strings)\n",
        "# NOTE: NLTK automatically calls nltk.tokenize.sent_tokenize and \n",
        "#       nltk.tokenize.word_tokenize.\n",
        "#\n",
        "# Each element in the outermost list is a paragraph, and\n",
        "# Each paragraph contains sentence(s), and\n",
        "# Each sentence contains token(s)\n",
        "print(newcorpus.paras())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZQuGzjJAhW0s",
        "outputId": "f62a2c0b-c947-46bc-e7d9-572736ef208c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['This', 'is', 'a', 'foo', 'bar', 'sentence', '.'], ['And', 'this', 'is', 'the', 'first', 'txtfile', 'in', 'the', 'corpus', '.']], [['Are', 'you', 'a', 'foo', 'bar', '?'], ['Yes', 'I', 'am', '.'], ['Possibly', ',', 'everyone', 'is', '.']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To access pargraphs of a specific fileid.\n",
        "print(newcorpus.paras(newcorpus.fileids()[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocsya1dNhXak",
        "outputId": "2174d6ce-70c7-4a8f-af85-c79a547aec0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[['This', 'is', 'a', 'foo', 'bar', 'sentence', '.'], ['And', 'this', 'is', 'the', 'first', 'txtfile', 'in', 'the', 'corpus', '.']]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access sentences in the corpus. (list of list of strings)\n",
        "# NOTE: That the texts are flattened into sentences that contains tokens.\n",
        "print(newcorpus.sents())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJx3y9JchX2r",
        "outputId": "40b20821-3efe-4293-f0d6-ed20ce249666"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['This', 'is', 'a', 'foo', 'bar', 'sentence', '.'], ['And', 'this', 'is', 'the', 'first', 'txtfile', 'in', 'the', 'corpus', '.'], ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To access sentences of a specific fileid.\n",
        "print(newcorpus.sents(newcorpus.fileids()[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-J19OnehYRk",
        "outputId": "11b0a881-b415-49d4-fff0-848c6229e096"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['This', 'is', 'a', 'foo', 'bar', 'sentence', '.'], ['And', 'this', 'is', 'the', 'first', 'txtfile', 'in', 'the', 'corpus', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access just tokens/words in the corpus. (list of strings)\n",
        "print(newcorpus.words())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKVkm_BnhYxo",
        "outputId": "5f644074-8f6d-42cc-de5d-b1a23d6e4388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'foo', 'bar', 'sentence', '.', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To access tokens of a specific fileid.\n",
        "print(newcorpus.words(newcorpus.fileids()[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yAVfj5Qzk42J",
        "outputId": "8da9c0ea-0c72-4bf5-fb81-3772da9d519c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'a', 'foo', 'bar', 'sentence', '.', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To access tokens of a specific fileid.\n",
        "print(newcorpus.words(newcorpus.fileids()[1]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42H06D96k7cx",
        "outputId": "492af774-7b6c-4df7-b065-f4e2c5121805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are', 'you', 'a', 'foo', 'bar', '?', 'Yes', 'I', ...]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqed3ELjk_iD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}