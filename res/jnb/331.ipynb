{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOAgUUJc5zFM1S8EKyDAjwQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_r-eMlMgOrv","executionInfo":{"status":"ok","timestamp":1666171655319,"user_tz":-120,"elapsed":6285,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"3a18da22-513e-499f-b49b-cc44065c38ce"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading collection 'popular'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection popular\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}],"source":["import os\n","import nltk\n","nltk.download(\"popular\")"]},{"cell_type":"code","source":["# Let's create a corpus with 2 texts in different textfile.\n","txt1 = \"\"\"This is a foo bar sentence.\\nAnd this is the first txtfile in the corpus.\"\"\"\n","txt2 = \"\"\"Are you a foo bar? Yes I am. Possibly, everyone is.\\n\"\"\"\n","corpus = [txt1,txt2]\n","\n","# Make new dir for the corpus.\n","corpusdir = 'tmp/newcorpus/'\n","if not os.path.isdir(corpusdir):\n","    os.mkdir(corpusdir)\n","\n","# Output the files into the directory.\n","filename = 0\n","for text in corpus:\n","    filename+=1\n","    with open(corpusdir+str(filename)+'.txt','w') as fout:\n","        #print>>fout, text\n","        #print(text, fout)\n","        fout.write(text)\n"],"metadata":{"id":"z0OtxEf_hVW1","executionInfo":{"status":"ok","timestamp":1666171386311,"user_tz":-120,"elapsed":290,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["!cat newcorpus/1.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Li6wtDUi1TS","executionInfo":{"status":"ok","timestamp":1666171389608,"user_tz":-120,"elapsed":827,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"91335ee1-716d-4e9d-cfdc-d4d3579f4f97"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["This is a foo bar sentence.\n","And this is the first txtfile in the corpus."]}]},{"cell_type":"code","source":["# Check that our corpus do exist and the files are correct.\n","assert os.path.isdir(corpusdir)\n","for infile, text in zip(sorted(os.listdir(corpusdir)),corpus):\n","    assert open(corpusdir+infile,'r').read().strip() == text.strip()\n","\n","\n","from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n","# Create a new corpus by specifying the parameters\n","# (1) directory of the new corpus\n","# (2) the fileids of the corpus\n","# NOTE: in this case the fileids are simply the filenames.\n","newcorpus = PlaintextCorpusReader('newcorpus/', '.*')\n"],"metadata":{"id":"EfEHvPwDhV3f","executionInfo":{"status":"ok","timestamp":1666171457283,"user_tz":-120,"elapsed":303,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Access each file in the corpus.\n","for infile in sorted(newcorpus.fileids()):\n","    print(infile) # The fileids of each file.\n","    with newcorpus.open(infile) as fin: # Opens the file.\n","        print(fin.read().strip()) # Prints the content of the file\n","\n","# Access the plaintext; outputs pure string/basestring.\n","print(newcorpus.raw().strip())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rfIgEXPxhWZC","executionInfo":{"status":"ok","timestamp":1666171540238,"user_tz":-120,"elapsed":253,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"bfee8af6-a937-484c-b1af-f582ef681ad7"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["1.txt\n","This is a foo bar sentence.\n","And this is the first txtfile in the corpus.\n","2.txt\n","Are you a foo bar? Yes I am. Possibly, everyone is.\n","This is a foo bar sentence.\n","And this is the first txtfile in the corpus.Are you a foo bar? Yes I am. Possibly, everyone is.\n"]}]},{"cell_type":"code","source":["# Access paragraphs in the corpus. (list of list of list of strings)\n","# NOTE: NLTK automatically calls nltk.tokenize.sent_tokenize and \n","#       nltk.tokenize.word_tokenize.\n","#\n","# Each element in the outermost list is a paragraph, and\n","# Each paragraph contains sentence(s), and\n","# Each sentence contains token(s)\n","print(newcorpus.paras())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZQuGzjJAhW0s","executionInfo":{"status":"ok","timestamp":1666171682478,"user_tz":-120,"elapsed":627,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"f62a2c0b-c947-46bc-e7d9-572736ef208c"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["[[['This', 'is', 'a', 'foo', 'bar', 'sentence', '.'], ['And', 'this', 'is', 'the', 'first', 'txtfile', 'in', 'the', 'corpus', '.']], [['Are', 'you', 'a', 'foo', 'bar', '?'], ['Yes', 'I', 'am', '.'], ['Possibly', ',', 'everyone', 'is', '.']]]\n"]}]},{"cell_type":"code","source":["# To access pargraphs of a specific fileid.\n","print(newcorpus.paras(newcorpus.fileids()[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ocsya1dNhXak","executionInfo":{"status":"ok","timestamp":1666171713195,"user_tz":-120,"elapsed":214,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"2174d6ce-70c7-4a8f-af85-c79a547aec0f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[[['This', 'is', 'a', 'foo', 'bar', 'sentence', '.'], ['And', 'this', 'is', 'the', 'first', 'txtfile', 'in', 'the', 'corpus', '.']]]\n"]}]},{"cell_type":"code","source":["# Access sentences in the corpus. (list of list of strings)\n","# NOTE: That the texts are flattened into sentences that contains tokens.\n","print(newcorpus.sents())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BJx3y9JchX2r","executionInfo":{"status":"ok","timestamp":1666171734494,"user_tz":-120,"elapsed":317,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"40b20821-3efe-4293-f0d6-ed20ce249666"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[['This', 'is', 'a', 'foo', 'bar', 'sentence', '.'], ['And', 'this', 'is', 'the', 'first', 'txtfile', 'in', 'the', 'corpus', '.'], ...]\n"]}]},{"cell_type":"code","source":["# To access sentences of a specific fileid.\n","print(newcorpus.sents(newcorpus.fileids()[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y-J19OnehYRk","executionInfo":{"status":"ok","timestamp":1666171756507,"user_tz":-120,"elapsed":416,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"11b0a881-b415-49d4-fff0-848c6229e096"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["[['This', 'is', 'a', 'foo', 'bar', 'sentence', '.'], ['And', 'this', 'is', 'the', 'first', 'txtfile', 'in', 'the', 'corpus', '.']]\n"]}]},{"cell_type":"code","source":["# Access just tokens/words in the corpus. (list of strings)\n","print(newcorpus.words())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FKVkm_BnhYxo","executionInfo":{"status":"ok","timestamp":1666171769102,"user_tz":-120,"elapsed":313,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"5f644074-8f6d-42cc-de5d-b1a23d6e4388"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'foo', 'bar', 'sentence', '.', ...]\n"]}]},{"cell_type":"code","source":["# To access tokens of a specific fileid.\n","print(newcorpus.words(newcorpus.fileids()[0]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yAVfj5Qzk42J","executionInfo":{"status":"ok","timestamp":1666171779751,"user_tz":-120,"elapsed":315,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"8da9c0ea-0c72-4bf5-fb81-3772da9d519c"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["['This', 'is', 'a', 'foo', 'bar', 'sentence', '.', ...]\n"]}]},{"cell_type":"code","source":["# To access tokens of a specific fileid.\n","print(newcorpus.words(newcorpus.fileids()[1]))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42H06D96k7cx","executionInfo":{"status":"ok","timestamp":1666171796241,"user_tz":-120,"elapsed":316,"user":{"displayName":"Nils Holmberg","userId":"03719824180037236130"}},"outputId":"492af774-7b6c-4df7-b065-f4e2c5121805"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["['Are', 'you', 'a', 'foo', 'bar', '?', 'Yes', 'I', ...]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"tqed3ELjk_iD"},"execution_count":null,"outputs":[]}]}